# Capstone-AutoScout-Project
Welcome to "AutoScout Data Analysis Project". This is the capstone project of Data Analysis Module. Auto Scout data which using for this project, scraped from the on-line car trading company in 2019, contains many features of 9 different car models. In this project, you will have the opportunity to apply many commonly used algorithms for Data Cleaning and Exploratory Data Analysis by using many Python libraries such as Numpy, Pandas, Matplotlib, Seaborn, Scipy you will analyze clean dataset.
Some Reminders on Exploratory data analysis (EDA)
Exploratory data analysis (EDA) is an especially important activity in the routine of a data analyst or scientist. It enables an in depth understanding of the dataset, define or discard hypotheses and create predictive models on a solid basis. It uses data manipulation techniques and several statistical tools to describe and understand the relationship between variables and how these can impact business. By means of EDA, we can obtain meaningful insights that can impact analysis under the following questions (If a checklist is good enough for pilots to use every flight, it’s good enough for data scientists to use with every dataset).

What question are you trying to solve (or prove wrong)?
What kind of data do you have?
What’s missing from the data?
Where are the outliers?
How can you add, change or remove features to get more out of your data?
Exploratory data analysis (EDA) is often an iterative brainstorming process where you pose a question, review the data, and develop further questions to investigate before beginning model development work. The image below shows how the brainstorming phase is connected with that of understanding the variables and how this in turn is connected again with the brainstorming phase.




In this context, the project consists of 3 parts in general:

The first part is related to 'Data Cleaning'. It deals with Incorrect Headers, Incorrect Format, Anomalies, and Dropping useless columns.
The second part is related to 'Filling Data', in other words 'Imputation'. It deals with Missing Values. Categorical to numeric transformation is done as well.
The third part is related to 'Handling Outliers of Data' via Visualization libraries. So, some insights will be extracted.
NOTE: However, you are free to create your own style. You do NOT have to stick to the steps above. We, the DA & DV instructors, recommend you study each part separately to create a source notebook for each part title for your further studies.
